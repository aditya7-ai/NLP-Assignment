{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwU0Bmw9hI_p",
        "outputId": "3c37d929-5ecd-4fa6-8d6c-d1d5520a2368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello! My name is Rushikesh. I am from Nashik, New York is city. I like running. Natural Language Processing :) #AI\"\n"
      ],
      "metadata": {
        "id": "38p5PUtvhlah"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(wt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBIOYglEiEEB",
        "outputId": "77b4de9b-3b54-49d7-8ad3-4b0fdea09cc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello!', 'My', 'name', 'is', 'Rushikesh.', 'I', 'am', 'from', 'Nashik,', 'New', 'York', 'is', 'city.', 'I', 'like', 'running.', 'Natural', 'Language', 'Processing', ':)', '#AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(wordpunct_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLav_s1BiMKE",
        "outputId": "0faf2517-5c1a-4cc6-d5f2-dcb3e37da02b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Rushikesh', '.', 'I', 'am', 'from', 'Nashik', ',', 'New', 'York', 'is', 'city', '.', 'I', 'like', 'running', '.', 'Natural', 'Language', 'Processing', ':)', '#', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "twt = TreebankWordTokenizer()\n",
        "print(twt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lniTJKsliUEv",
        "outputId": "493cd064-df11-4111-ed49-37ed07bd80b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Rushikesh.', 'I', 'am', 'from', 'Nashik', ',', 'New', 'York', 'is', 'city.', 'I', 'like', 'running.', 'Natural', 'Language', 'Processing', ':', ')', '#', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(tweet_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFfBu7twiapX",
        "outputId": "45c04e04-797d-4851-c915-dac59eb94ad0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Rushikesh', '.', 'I', 'am', 'from', 'Nashik', ',', 'New', 'York', 'is', 'city', '.', 'I', 'like', 'running', '.', 'Natural', 'Language', 'Processing', ':)', '#AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('New', 'York'), ('Natural', 'Language', 'Processing')])\n",
        "print(mwe.tokenize(wordpunct_tokenize(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5RyGgFVi1Fg",
        "outputId": "1c3ee4e5-1b21-44f2-97dc-173f36e8ef4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Rushikesh', '.', 'I', 'am', 'from', 'Nashik', ',', 'New_York', 'is', 'city', '.', 'I', 'like', 'running', '.', 'Natural_Language_Processing', ':)', '#', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "tokens = wordpunct_tokenize(text)\n",
        "\n",
        "print([ps.stem(word) for word in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ky7t_bVjMWD",
        "outputId": "23c51b2d-3b37-4a0f-d064-75ec4aa1ae4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'my', 'name', 'is', 'rushikesh', '.', 'i', 'am', 'from', 'nashik', ',', 'new', 'york', 'is', 'citi', '.', 'i', 'like', 'run', '.', 'natur', 'languag', 'process', ':)', '#', 'ai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print([ss.stem(word) for word in tokens])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0MTLk9gjbWy",
        "outputId": "1d4d767c-ddb4-4a27-a71f-cd7a6d5f5bed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'my', 'name', 'is', 'rushikesh', '.', 'i', 'am', 'from', 'nashik', ',', 'new', 'york', 'is', 'citi', '.', 'i', 'like', 'run', '.', 'natur', 'languag', 'process', ':)', '#', 'ai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print([lemmatizer.lemmatize(word.lower()) for word in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npaf-A-WjisD",
        "outputId": "63c2e796-b188-4372-df62-bab8e81119e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'my', 'name', 'is', 'rushikesh', '.', 'i', 'am', 'from', 'nashik', ',', 'new', 'york', 'is', 'city', '.', 'i', 'like', 'running', '.', 'natural', 'language', 'processing', ':)', '#', 'ai']\n"
          ]
        }
      ]
    }
  ]
}