{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwU0Bmw9hI_p",
        "outputId": "5da35e8c-4abc-4dd0-c4f9-d0073722e325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "38p5PUtvhlah"
      },
      "outputs": [],
      "source": [
        "text = \"Hello! My name is Aditya . I am from Akluj, New York is city. I like running. Natural Language Processing :) #AI\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBIOYglEiEEB",
        "outputId": "dba5cb82-3e3a-4e90-f24a-f27e4e2ab349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello!', 'My', 'name', 'is', 'Aditya', '.', 'I', 'am', 'from', 'Akluj,', 'New', 'York', 'is', 'city.', 'I', 'like', 'running.', 'Natural', 'Language', 'Processing', ':)', '#AI']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(wt.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLav_s1BiMKE",
        "outputId": "9faedb8d-32f4-4356-d49d-5cc37c495d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Aditya', '.', 'I', 'am', 'from', 'Akluj', ',', 'New', 'York', 'is', 'city', '.', 'I', 'like', 'running', '.', 'Natural', 'Language', 'Processing', ':)', '#', 'AI']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(wordpunct_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lniTJKsliUEv",
        "outputId": "3b7a0afc-0c9a-4053-8088-9fb7bf2f3daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Aditya', '.', 'I', 'am', 'from', 'Akluj', ',', 'New', 'York', 'is', 'city.', 'I', 'like', 'running.', 'Natural', 'Language', 'Processing', ':', ')', '#', 'AI']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "twt = TreebankWordTokenizer()\n",
        "print(twt.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFfBu7twiapX",
        "outputId": "d047aa24-0d02-423c-8ba2-4f4ce6ff56fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Aditya', '.', 'I', 'am', 'from', 'Akluj', ',', 'New', 'York', 'is', 'city', '.', 'I', 'like', 'running', '.', 'Natural', 'Language', 'Processing', ':)', '#AI']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(tweet_tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5RyGgFVi1Fg",
        "outputId": "58aaeedd-8f07-410a-8e76-ef2d46110e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Aditya', '.', 'I', 'am', 'from', 'Akluj', ',', 'New_York', 'is', 'city', '.', 'I', 'like', 'running', '.', 'Natural_Language_Processing', ':)', '#', 'AI']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('New', 'York'), ('Natural', 'Language', 'Processing')])\n",
        "print(mwe.tokenize(wordpunct_tokenize(text)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ky7t_bVjMWD",
        "outputId": "ef176ee7-db2d-4be1-8452-74360b978870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'my', 'name', 'is', 'aditya', '.', 'i', 'am', 'from', 'akluj', ',', 'new', 'york', 'is', 'citi', '.', 'i', 'like', 'run', '.', 'natur', 'languag', 'process', ':)', '#', 'ai']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "tokens = wordpunct_tokenize(text)\n",
        "\n",
        "print([ps.stem(word) for word in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0MTLk9gjbWy",
        "outputId": "0a987d11-fee3-44b5-b24a-c82711a40cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'my', 'name', 'is', 'aditya', '.', 'i', 'am', 'from', 'akluj', ',', 'new', 'york', 'is', 'citi', '.', 'i', 'like', 'run', '.', 'natur', 'languag', 'process', ':)', '#', 'ai']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print([ss.stem(word) for word in tokens])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npaf-A-WjisD",
        "outputId": "f8fd9c3a-6dc7-4b7d-e594-a7ace4c39fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'my', 'name', 'is', 'aditya', '.', 'i', 'am', 'from', 'akluj', ',', 'new', 'york', 'is', 'city', '.', 'i', 'like', 'running', '.', 'natural', 'language', 'processing', ':)', '#', 'ai']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print([lemmatizer.lemmatize(word.lower()) for word in tokens])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}